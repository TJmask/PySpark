{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark Machine Learning on Credit Card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext \n",
    "from pyspark import SQLContext\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.7.29:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ysh_PySparkML</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=ysh_PySparkML>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext(appName ='ysh_PySparkML', master = 'local')\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x112ceae50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## build the sqlContext \n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# UniversalBank = pd.read_csv('/Users/tjmask/Desktop/Semester2/Spark/PySpark/Datasets/UniversalBank.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data and choose features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------+------+--------+------+-----+---------+--------+-------------+------------------+----------+------+----------+\n",
      "| ID|Age|Experience|Income|ZIP Code|Family|CCAvg|Education|Mortgage|Personal Loan|Securities Account|CD Account|Online|CreditCard|\n",
      "+---+---+----------+------+--------+------+-----+---------+--------+-------------+------------------+----------+------+----------+\n",
      "|  1| 25|         1|    49|   91107|     4|  1.6|        1|       0|            0|                 1|         0|     0|         0|\n",
      "|  2| 45|        19|    34|   90089|     3|  1.5|        1|       0|            0|                 1|         0|     0|         0|\n",
      "|  3| 39|        15|    11|   94720|     1|  1.0|        1|       0|            0|                 0|         0|     0|         0|\n",
      "|  4| 35|         9|   100|   94112|     1|  2.7|        2|       0|            0|                 0|         0|     0|         0|\n",
      "|  5| 35|         8|    45|   91330|     4|  1.0|        2|       0|            0|                 0|         0|     0|         1|\n",
      "+---+---+----------+------+--------+------+-----+---------+--------+-------------+------------------+----------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## load data\n",
    "path = 'file:///Users/tjmask/Desktop/Semester2/Spark/PySpark/Datasets/UniversalBank.csv'\n",
    "UniversalBank = sqlContext.read.csv(path, header=True, inferSchema=True)\n",
    "UniversalBank.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'Age',\n",
       " 'Experience',\n",
       " 'Income',\n",
       " 'ZIP Code',\n",
       " 'Family',\n",
       " 'CCAvg',\n",
       " 'Education',\n",
       " 'Mortgage',\n",
       " 'Personal Loan',\n",
       " 'Securities Account',\n",
       " 'CD Account',\n",
       " 'Online',\n",
       " 'CreditCard']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UniversalBank.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CreditCard is the label, it has two kinds of values, 0: credit card failed, 1: credict card passed. We also see that ID and ZIP code are not helpful for predicting, we will move those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UniversalBank.createOrReplaceTempView(\"df_Bank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age',\n",
       " 'Experience',\n",
       " 'Income',\n",
       " 'Family',\n",
       " 'CCAvg',\n",
       " 'Education',\n",
       " 'Mortgage',\n",
       " 'Personal Loan',\n",
       " 'Securities Account',\n",
       " 'CD Account',\n",
       " 'Online',\n",
       " 'CreditCard']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## select the useful features\n",
    "selected_features = [c for c in UniversalBank.columns if c not in ['ZIP Code', 'ID']]\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is our selected feautures used for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------+------+-----+---------+--------+-------------+------------------+----------+------+----------+\n",
      "|Age|Experience|Income|Family|CCAvg|Education|Mortgage|Personal Loan|Securities Account|CD Account|Online|CreditCard|\n",
      "+---+----------+------+------+-----+---------+--------+-------------+------------------+----------+------+----------+\n",
      "| 25|         1|    49|     4|  1.6|        1|       0|            0|                 1|         0|     0|         0|\n",
      "| 45|        19|    34|     3|  1.5|        1|       0|            0|                 1|         0|     0|         0|\n",
      "| 39|        15|    11|     1|  1.0|        1|       0|            0|                 0|         0|     0|         0|\n",
      "| 35|         9|   100|     1|  2.7|        2|       0|            0|                 0|         0|     0|         0|\n",
      "| 35|         8|    45|     4|  1.0|        2|       0|            0|                 0|         0|     0|         1|\n",
      "+---+----------+------+------+-----+---------+--------+-------------+------------------+----------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## get the trimmed dataset\n",
    "UniversalBank_trimmed = UniversalBank.select(selected_features)\n",
    "UniversalBank_trimmed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age: \t45.34 \t 11.46\n",
      "Experience: \t20.10 \t 11.47\n",
      "Income: \t73.77 \t 46.03\n",
      "Family: \t2.40 \t 1.15\n",
      "CCAvg: \t1.94 \t 1.75\n",
      "Education: \t1.88 \t 0.84\n",
      "Mortgage: \t56.50 \t 101.71\n",
      "Personal Loan: \t0.10 \t 0.29\n",
      "Securities Account: \t0.10 \t 0.31\n",
      "CD Account: \t0.06 \t 0.24\n",
      "Online: \t0.60 \t 0.49\n",
      "CreditCard: \t0.29 \t 0.46\n"
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.stat as st\n",
    "import numpy as np\n",
    "\n",
    "numeric_rdd = UniversalBank_trimmed.rdd.map(lambda row: [e for e in row])\n",
    "\n",
    "mllib_stats = st.Statistics.colStats(numeric_rdd)\n",
    "\n",
    "mllib_stats\n",
    "\n",
    "for col, m, v in zip(selected_features, \n",
    "                     mllib_stats.mean(), \n",
    "                     mllib_stats.variance()):\n",
    "    print('{0}: \\t{1:.2f} \\t {2:.2f}'.format(col, m, np.sqrt(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+\n",
      "|summary|               Age|       Experience|           Income|            Family|             CCAvg|        Education|          Mortgage|      Personal Loan| Securities Account|         CD Account|            Online|        CreditCard|\n",
      "+-------+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+\n",
      "|  count|              5000|             5000|             5000|              5000|              5000|             5000|              5000|               5000|               5000|               5000|              5000|              5000|\n",
      "|   mean|           45.3384|          20.1046|          73.7742|            2.3964|1.9379380000000053|            1.881|           56.4988|              0.096|             0.1044|             0.0604|            0.5968|             0.294|\n",
      "| stddev|11.463165630542662|11.46795368112056|46.03372932108627|1.1476630455378507|1.7476589800467675|0.839869082664199|101.71380210211213|0.29462070577617994|0.30580932600032634|0.23825027311322794|0.4905893349626712|0.4556374886949281|\n",
      "|    min|                23|               -3|                8|                 1|               0.0|                1|                 0|                  0|                  0|                  0|                 0|                 0|\n",
      "|    max|                67|               43|              224|                 4|              10.0|                3|               635|                  1|                  1|                  1|                 1|                 1|\n",
      "+-------+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "UniversalBank_trimmed.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correlations between features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age-to-Experience: 0.99\n",
      "Experience-to-Age: 0.99\n",
      "Income-to-CCAvg: 0.65\n",
      "Income-to-Personal Loan: 0.50\n",
      "CCAvg-to-Income: 0.65\n",
      "Personal Loan-to-Income: 0.50\n"
     ]
    }
   ],
   "source": [
    "corrs = st.Statistics.corr(numeric_rdd)\n",
    "\n",
    "for i, el in enumerate(corrs > 0.5):\n",
    "    correlated = [\n",
    "        (selected_features[j], corrs[i][j]) \n",
    "        for j, e in enumerate(el) \n",
    "        if e == 1.0 and j != i]\n",
    "    \n",
    "    if len(correlated) > 0:\n",
    "        for e in correlated:\n",
    "            print('{0}-to-{1}: {2:.2f}' \\\n",
    "                  .format(selected_features[i], e[0], e[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop most of highly correlated features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save data, reread, transform use LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.mllib.feature as ft\n",
    "import pyspark.mllib.regression as reg\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UniversalBank_trimmed.rdd.map(lambda arr:[arr[c] for c in range(12)]).\\\n",
    "#             saveAsTextFile('/Users/tjmask/Desktop/Semester2/Spark/PySpark/Datasets/UniversalBank_trimmed', compressionCodecClass=int)\n",
    "    \n",
    "# points = sc.textFile('file:///Users/tjmask/Desktop/Semester2/Spark/PySpark/Datasets/UniversalBank_trimmed')\n",
    "# points.map(lambda line: int(line))\n",
    "# points.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "## transform to RDD\n",
    "points = UniversalBank_trimmed.rdd.map(lambda arr:[arr[c] for c in range(12)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_point(line):\n",
    "#     values = [s for s in line.strip().split(',')]\n",
    "    return LabeledPoint(line[11], line[0:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [25.0,1.0,49.0,4.0,1.6,1.0,0.0,0.0,1.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [45.0,19.0,34.0,3.0,1.5,1.0,0.0,0.0,1.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [39.0,15.0,11.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [35.0,9.0,100.0,1.0,2.7,2.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(1.0, [35.0,8.0,45.0,4.0,1.0,2.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_parsed = points.map(parse_point)\n",
    "data_parsed.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data_parsed.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification \\\n",
    "    import LogisticRegressionWithLBFGS\n",
    "\n",
    "LR_Model = LogisticRegressionWithLBFGS \\\n",
    "    .train(train, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_results = (\n",
    "        test.map(lambda row: row.label) \\\n",
    "        .zip(LR_Model.predict(test.map(lambda row: row.features)))\n",
    "    ).map(lambda row: (row[0], row[1] * 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0), (1.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_results.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR: 0.13\n",
      "Area under ROC: 0.72\n"
     ]
    }
   ],
   "source": [
    "import pyspark.mllib.evaluation as ev\n",
    "LR_evaluation = ev.BinaryClassificationMetrics(LR_results)\n",
    "\n",
    "print('Area under PR: {0:.2f}' \\\n",
    "      .format(LR_evaluation.areaUnderPR))\n",
    "print('Area under ROC: {0:.2f}' \\\n",
    "      .format(LR_evaluation.areaUnderROC))\n",
    "LR_evaluation.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7491408934707904\n"
     ]
    }
   ],
   "source": [
    "Accuracy = LR_results.filter(lambda x: x[0]==x[1]).count()/(LR_results.count())\n",
    "print('Accuracy: ', Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest with original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "\n",
    "RF_model = RandomForest \\\n",
    "    .trainClassifier(data=train, \n",
    "                     numClasses=2, \n",
    "                     categoricalFeaturesInfo={}, \n",
    "                     numTrees=6,  \n",
    "                     featureSubsetStrategy='all',\n",
    "                     seed=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR: 0.15\n",
      "Area under ROC: 0.81\n"
     ]
    }
   ],
   "source": [
    "RF_results = (\n",
    "        test.map(lambda row: row.label) \\\n",
    "        .zip(RF_model \\\n",
    "             .predict(test \\\n",
    "                      .map(lambda row: row.features)))\n",
    "    )\n",
    "\n",
    "RF_evaluation = ev.BinaryClassificationMetrics(RF_results)\n",
    "\n",
    "print('Area under PR: {0:.2f}' \\\n",
    "      .format(RF_evaluation.areaUnderPR))\n",
    "print('Area under ROC: {0:.2f}' \\\n",
    "      .format(RF_evaluation.areaUnderROC))\n",
    "RF_evaluation.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7450171821305842\n"
     ]
    }
   ],
   "source": [
    "Accuracy = RF_results.filter(lambda x: x[0]==x[1]).count()/(RF_results.count())\n",
    "print('Accuracy: ', Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting only the most predictable featuresÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ft.ChiSqSelector(4).fit(train)\n",
    "\n",
    "topFeatures_train = (\n",
    "        train.map(lambda row: row.label) \\\n",
    "        .zip(selector \\\n",
    "             .transform(train \\\n",
    "                        .map(lambda row: row.features)))\n",
    "    ).map(lambda row: reg.LabeledPoint(row[0], row[1]))\n",
    "\n",
    "topFeatures_test = (\n",
    "        test.map(lambda row: row.label) \\\n",
    "        .zip(selector \\\n",
    "             .transform(test \\\n",
    "                        .map(lambda row: row.features)))\n",
    "    ).map(lambda row: reg.LabeledPoint(row[0], row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [37.0,13.0,0.0,0.0])]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topFeatures_test.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest with reduced features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest\n",
    "\n",
    "RF_model_2 = RandomForest \\\n",
    "    .trainClassifier(data=topFeatures_train, \n",
    "                     numClasses=2, \n",
    "                     categoricalFeaturesInfo={}, \n",
    "                     numTrees=6,  \n",
    "                     featureSubsetStrategy='all',\n",
    "                     seed=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR: 0.10\n",
      "Area under ROC: 0.66\n"
     ]
    }
   ],
   "source": [
    "RF_results_2 = (\n",
    "        topFeatures_test.map(lambda row: row.label) \\\n",
    "        .zip(RF_model_2 \\\n",
    "             .predict(topFeatures_test \\\n",
    "                      .map(lambda row: row.features)))\n",
    "    )\n",
    "\n",
    "RF_evaluation_2 = ev.BinaryClassificationMetrics(RF_results_2)\n",
    "\n",
    "print('Area under PR: {0:.2f}' \\\n",
    "      .format(RF_evaluation_2.areaUnderPR))\n",
    "print('Area under ROC: {0:.2f}' \\\n",
    "      .format(RF_evaluation_2.areaUnderROC))\n",
    "RF_evaluation_2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0), (0.0, 0.0)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_results_2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.727147766323024\n"
     ]
    }
   ],
   "source": [
    "Accuracy = RF_results_2.filter(lambda x: x[0]==x[1]).count()/(RF_results_2.count())\n",
    "print('Accuracy: ', Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logit with reduced featrues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR: 0.12\n",
      "Area under ROC: 0.71\n"
     ]
    }
   ],
   "source": [
    "LR_Model_2 = LogisticRegressionWithLBFGS \\\n",
    "    .train(topFeatures_train, iterations=10)\n",
    "\n",
    "LR_results_2 = (\n",
    "        topFeatures_test.map(lambda row: row.label) \\\n",
    "        .zip(LR_Model_2 \\\n",
    "             .predict(topFeatures_test \\\n",
    "                      .map(lambda row: row.features)))\n",
    "    ).map(lambda row: (row[0], row[1] * 1.0))\n",
    "\n",
    "LR_evaluation_2 = ev.BinaryClassificationMetrics(LR_results_2)\n",
    "\n",
    "print('Area under PR: {0:.2f}' \\\n",
    "      .format(LR_evaluation_2.areaUnderPR))\n",
    "print('Area under ROC: {0:.2f}' \\\n",
    "      .format(LR_evaluation_2.areaUnderROC))\n",
    "LR_evaluation_2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7429553264604811\n"
     ]
    }
   ],
   "source": [
    "Accuracy = LR_results_2.filter(lambda x: x[0]==x[1]).count()/(LR_results_2.count())\n",
    "print('Accuracy: ', Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get dummies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "| ID|TYPE|CODE|\n",
      "+---+----+----+\n",
      "|  1|   A|  X1|\n",
      "|  2|   B|  X2|\n",
      "|  3|   B|  X3|\n",
      "|  1|   B|  X3|\n",
      "|  2|   C|  X2|\n",
      "|  3|   C|  X2|\n",
      "|  1|   C|  X1|\n",
      "|  1|   B|  X1|\n",
      "+---+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df = sqlContext.createDataFrame([\n",
    "    (1, \"A\", \"X1\"),\n",
    "    (2, \"B\", \"X2\"),\n",
    "    (3, \"B\", \"X3\"),\n",
    "    (1, \"B\", \"X3\"),\n",
    "    (2, \"C\", \"X2\"),\n",
    "    (3, \"C\", \"X2\"),\n",
    "    (1, \"C\", \"X1\"),\n",
    "    (1, \"B\", \"X1\"),\n",
    "], [\"ID\", \"TYPE\", \"CODE\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "# types = df.select(\"TYPE\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "# codes = df.select(\"CODE\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "# types_expr = [F.when(F.col(\"TYPE\") == ty, 1).otherwise(0).alias(\"e_TYPE_\" + ty) for ty in types]\n",
    "# codes_expr = [F.when(F.col(\"CODE\") == code, 1).otherwise(0).alias(\"e_CODE_\" + code) for code in codes]\n",
    "# df = df.select(\"ID\", \"TYPE\", \"CODE\", *types_expr+codes_expr)\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = df.select('TYPE').distinct().rdd.flatMap(lambda x:x).collect()\n",
    "codes = df.select('CODE').distinct().rdd.flatMap(lambda x:x).collect()\n",
    "types_dummy = [F.when(F.col('TYPE') == ty, 1).otherwise(0).alias('e_TYPE_'+ty) for ty in types]\n",
    "codes_dummy = [F.when(F.col('CODE') == code, 1).otherwise(0).alias('e_TYPE_'+code) for code in codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+--------+--------+--------+---------+---------+---------+\n",
      "| ID|TYPE|CODE|e_TYPE_B|e_TYPE_C|e_TYPE_A|e_TYPE_X1|e_TYPE_X3|e_TYPE_X2|\n",
      "+---+----+----+--------+--------+--------+---------+---------+---------+\n",
      "|  1|   A|  X1|       0|       0|       1|        1|        0|        0|\n",
      "|  2|   B|  X2|       1|       0|       0|        0|        0|        1|\n",
      "|  3|   B|  X3|       1|       0|       0|        0|        1|        0|\n",
      "|  1|   B|  X3|       1|       0|       0|        0|        1|        0|\n",
      "|  2|   C|  X2|       0|       1|       0|        0|        0|        1|\n",
      "|  3|   C|  X2|       0|       1|       0|        0|        0|        1|\n",
      "|  1|   C|  X1|       0|       1|       0|        1|        0|        0|\n",
      "|  1|   B|  X1|       1|       0|       0|        1|        0|        0|\n",
      "+---+----+----+--------+--------+--------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['*']+types_dummy+codes_dummy).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.feature as ft\n",
    "import pyspark.sql.types as typ\n",
    "\n",
    "df = df \\\n",
    "    .withColumn(       'TYPE_DUMMY', \n",
    "                df['TYPE'] \\\n",
    "                    .cast(typ.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----------+\n",
      "| ID|TYPE|CODE|TYPE_DUMMY|\n",
      "+---+----+----+----------+\n",
      "|  1|   A|  X1|      null|\n",
      "|  2|   B|  X2|      null|\n",
      "|  3|   B|  X3|      null|\n",
      "|  1|   B|  X3|      null|\n",
      "|  2|   C|  X2|      null|\n",
      "|  3|   C|  X2|      null|\n",
      "|  1|   C|  X1|      null|\n",
      "|  1|   B|  X1|      null|\n",
      "+---+----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ft.OneHotEncoder(\n",
    "    inputCol='TYPE_DUMMY', \n",
    "    outputCol='TYPE_DUMMY_VEC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ft.OneHotEncoder(inputCol=\"TYPE_DUMMY\", outputCol=\"TYPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    ('ID', typ.IntegerType()),\n",
    "    ('TYPE', typ.StringType()),\n",
    "    ('CODE', typ.StringType())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCreator = ft.VectorAssembler(\n",
    "    inputCols=[\n",
    "        col[0] \n",
    "        for col \n",
    "        in labels[2:]] + \\\n",
    "    [encoder.getOutputCol()], \n",
    "    outputCol='features'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_fbc933d5e95b"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.tuning as tune\n",
    "import pyspark.ml.classification as cl\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import pyspark.ml.evaluation as ev\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [25.0,1.0,49.0,4.0,1.6,1.0,0.0,0.0,1.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [45.0,19.0,34.0,3.0,1.5,1.0,0.0,0.0,1.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [39.0,15.0,11.0,1.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_parsed.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = cl.LogisticRegression(\n",
    "    labelCol='label')\n",
    "\n",
    "grid = tune.ParamGridBuilder() \\\n",
    "    .addGrid(logistic.maxIter,  \n",
    "             [2, 10, 50]) \\\n",
    "    .addGrid(logistic.regParam, \n",
    "             [0.01, 0.05, 0.3]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ev.BinaryClassificationEvaluator(\n",
    "    rawPredictionCol='probability', \n",
    "    labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = tune.CrossValidator(\n",
    "    estimator=logistic, \n",
    "    estimatorParamMaps=grid, \n",
    "    evaluator=evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[encoder,featuresCreator])\n",
    "data_transformer = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = pipeline.fit(train)\n",
    "# test_model = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxIter = [c for c in range(5,10)]\n",
    "# regParam = [0.001,0.01,0.1,1]\n",
    "# for i in maxIter:\n",
    "#     for j in regParam:\n",
    "#         logistic = LogisticRegression(maxIter=i, regParam=j) \n",
    "        \n",
    "#         LR = logistic.fit(train.toDF())\n",
    "\n",
    "#         LR_result = (\n",
    "#                 topFeatures_test.map(lambda row: row.label) \\\n",
    "#                 .zip(LR \\\n",
    "#                      .predict(test \\\n",
    "#                               .map(lambda row: row.features)))\n",
    "#             ).map(lambda row: (row[0], row[1] * 1.0))\n",
    "\n",
    "#         LR_evaluation = ev.BinaryClassificationMetrics(LR_result)\n",
    "\n",
    "#         print('MaxIter:{0}, reParam: {1}, Area under PR: {2:.2f}' \\\n",
    "#               .format(i,j,LR_evaluation.areaUnderPR))\n",
    "#         print('MaxIter:{0}, reParam: {1}, Area under ROC: {2:.2f}' \\\n",
    "#               .format(i,j, LR_evaluation.areaUnderROC))\n",
    "#         LR_evaluation.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|           features|label|\n",
      "+-------------------+-----+\n",
      "| [25.0,1.0,1.0,0.0]|  0.0|\n",
      "|[45.0,19.0,1.0,0.0]|  0.0|\n",
      "+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topFeatures_train.toDF().show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "bdf = sc.parallelize([\n",
    "Row(label=1.0, weight=1.0, features=Vectors.dense(0.0, 5.0)),\n",
    "Row(label=0.0, weight=2.0, features=Vectors.dense(1.0, 2.0)),\n",
    "Row(label=1.0, weight=3.0, features=Vectors.dense(2.0, 1.0)),\n",
    "Row(label=0.0, weight=4.0, features=Vectors.dense(3.0, 3.0))]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blor = LogisticRegression(regParam=0.01)\n",
    "# blorModel = blor.fit()\n",
    "# blorModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'PipelinedRDD' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-285-90ea340d6b20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnew_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArrayType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDoubleType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontainsNull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# LDA = LDA.withColumn(\"topic_vector_fix_dim\",udf_foo(\"topic_vector_fix_dim\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'PipelinedRDD' object is not callable"
     ]
    }
   ],
   "source": [
    "new_schema = ArrayType(DoubleType(), containsNull=False)\n",
    "x_new = X(lambda x:x, new_schema)\n",
    "# LDA = LDA.withColumn(\"topic_vector_fix_dim\",udf_foo(\"topic_vector_fix_dim\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import VectorAssembler\n",
    "# vectorAssembler = VectorAssembler(inputCols = [\"features\"], outputCol = \"features\")\n",
    "# df = vectorAssembler.transform(df)\n",
    "# df = df.select(['features', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = sqlContext.createDataFrame([\n",
    "    ['''Machine learning can be applied to a wide variety \n",
    "        of data types, such as vectors, text, images, and \n",
    "        structured data. This API adopts the DataFrame from \n",
    "        Spark SQL in order to support a variety of data types.'''],\n",
    "    ['''DataFrame supports many basic and structured types; \n",
    "        see the Spark SQL datatype reference for a list of \n",
    "        supported types. In addition to the types listed in \n",
    "        the Spark SQL guide, DataFrame can use ML Vector types.'''],\n",
    "    ['''A DataFrame can be created either implicitly or \n",
    "        explicitly from a regular RDD. See the code examples \n",
    "        below and the Spark SQL programming guide for examples.'''],\n",
    "    ['''Columns in a DataFrame are named. The code examples \n",
    "        below use names such as \"text,\" \"features,\" and \"label.\"''']\n",
    "], ['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               input|\n",
      "+--------------------+\n",
      "|Machine learning ...|\n",
      "|DataFrame support...|\n",
      "|A DataFrame can b...|\n",
      "|Columns in a Data...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ft.RegexTokenizer(\n",
    "    inputCol='input', \n",
    "    outputCol='input_arr', \n",
    "    pattern='\\s+|[,.\\\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(input_arr=['machine', 'learning', 'can', 'be', 'applied', 'to', 'a', 'wide', 'variety', 'of', 'data', 'types', 'such', 'as', 'vectors', 'text', 'images', 'and', 'structured', 'data', 'this', 'api', 'adopts', 'the', 'dataframe', 'from', 'spark', 'sql', 'in', 'order', 'to', 'support', 'a', 'variety', 'of', 'data', 'types']),\n",
       " Row(input_arr=['dataframe', 'supports', 'many', 'basic', 'and', 'structured', 'types;', 'see', 'the', 'spark', 'sql', 'datatype', 'reference', 'for', 'a', 'list', 'of', 'supported', 'types', 'in', 'addition', 'to', 'the', 'types', 'listed', 'in', 'the', 'spark', 'sql', 'guide', 'dataframe', 'can', 'use', 'ml', 'vector', 'types'])]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = tokenizer \\\n",
    "    .transform(text_data) \\\n",
    "    .select('input_arr') \n",
    "\n",
    "tok.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "locale = sc._jvm.java.util.Locale\n",
    "locale.setDefault(locale.forLanguageTag(\"en-US\"))\n",
    "stopwords = ft.StopWordsRemover(inputCol=tokenizer.getOutputCol(),\n",
    "                                outputCol='input_stop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(input_stop=['machine', 'learning', 'applied', 'wide', 'variety', 'data', 'types', 'vectors', 'text', 'images', 'structured', 'data', 'api', 'adopts', 'dataframe', 'spark', 'sql', 'order', 'support', 'variety', 'data', 'types'])]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.transform(tok).select('input_stop').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = ft.NGram(n=2, \n",
    "    inputCol=stopwords.getOutputCol(), \n",
    "    outputCol=\"nGrams\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords, ngram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(nGrams=['machine learning', 'learning applied', 'applied wide', 'wide variety', 'variety data', 'data types', 'types vectors', 'vectors text', 'text images', 'images structured', 'structured data', 'data api', 'api adopts', 'adopts dataframe', 'dataframe spark', 'spark sql', 'sql order', 'order support', 'support variety', 'variety data', 'data types'])]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ngram = pipeline \\\n",
    "    .fit(text_data) \\\n",
    "    .transform(text_data)\n",
    "    \n",
    "data_ngram.select('nGrams').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>delay</th>\n",
       "      <th>distance</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1011245</td>\n",
       "      <td>6</td>\n",
       "      <td>602</td>\n",
       "      <td>ABE</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1020600</td>\n",
       "      <td>-8</td>\n",
       "      <td>369</td>\n",
       "      <td>ABE</td>\n",
       "      <td>DTW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1021245</td>\n",
       "      <td>-2</td>\n",
       "      <td>602</td>\n",
       "      <td>ABE</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1020605</td>\n",
       "      <td>-4</td>\n",
       "      <td>602</td>\n",
       "      <td>ABE</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1031245</td>\n",
       "      <td>-4</td>\n",
       "      <td>602</td>\n",
       "      <td>ABE</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      date  delay  distance origin destination\n",
       "0  1011245      6       602    ABE         ATL\n",
       "1  1020600     -8       369    ABE         DTW\n",
       "2  1021245     -2       602    ABE         ATL\n",
       "3  1020605     -4       602    ABE         ATL\n",
       "4  1031245     -4       602    ABE         ATL"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/TJmask/learningPySpark/master/Data/departuredelays.csv'\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>delay</th>\n",
       "      <th>distance</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1011245</td>\n",
       "      <td>6</td>\n",
       "      <td>602</td>\n",
       "      <td>ABE</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1020600</td>\n",
       "      <td>-8</td>\n",
       "      <td>369</td>\n",
       "      <td>ABE</td>\n",
       "      <td>DTW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1021245</td>\n",
       "      <td>-2</td>\n",
       "      <td>602</td>\n",
       "      <td>ABE</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1020605</td>\n",
       "      <td>-4</td>\n",
       "      <td>602</td>\n",
       "      <td>ABE</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1031245</td>\n",
       "      <td>-4</td>\n",
       "      <td>602</td>\n",
       "      <td>ABE</td>\n",
       "      <td>ATL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      date  delay  distance origin destination\n",
       "0  1011245      6       602    ABE         ATL\n",
       "1  1020600     -8       369    ABE         DTW\n",
       "2  1021245     -2       602    ABE         ATL\n",
       "3  1020605     -4       602    ABE         ATL\n",
       "4  1031245     -4       602    ABE         ATL"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_data = requests.get(url).content\n",
    "address = pd.read_csv(io.StringIO(read_data.decode('utf-8')))\n",
    "address.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "      <th>IATA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abbotsford</td>\n",
       "      <td>BC</td>\n",
       "      <td>Canada</td>\n",
       "      <td>YXX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aberdeen</td>\n",
       "      <td>SD</td>\n",
       "      <td>USA</td>\n",
       "      <td>ABR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abilene</td>\n",
       "      <td>TX</td>\n",
       "      <td>USA</td>\n",
       "      <td>ABI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Akron</td>\n",
       "      <td>OH</td>\n",
       "      <td>USA</td>\n",
       "      <td>CAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alamosa</td>\n",
       "      <td>CO</td>\n",
       "      <td>USA</td>\n",
       "      <td>ALS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         City State Country IATA\n",
       "0  Abbotsford    BC  Canada  YXX\n",
       "1    Aberdeen    SD     USA  ABR\n",
       "2     Abilene    TX     USA  ABI\n",
       "3       Akron    OH     USA  CAK\n",
       "4     Alamosa    CO     USA  ALS"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path2 = 'https://raw.githubusercontent.com/TJmask/learningPySpark/master/Data/airport-codes-na.txt'\n",
    "df2 =  pd.read_csv(path2, sep=\"\\t\")##delimiter=\"\\t\", header=None\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('/Users/tjmask/Desktop/Semester2/Spark/PySpark/Datasets/departuredelays.csv')\n",
    "# df2.to_csv('/Users/tjmask/Desktop/Semester2/Spark/PySpark/Datasets/airport-codes-na.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
