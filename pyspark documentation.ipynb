{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "from pyspark import SparkContext \n",
    "from pyspark import SQLContext\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.7.29:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>First Lesson</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=First Lesson>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext(appName ='First Lesson', master = 'local')\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = sc.textFile('file:///Users/tjmask/Desktop/test.txt')\n",
    "path = 'file:///Users/tjmask/Desktop/Semester2/Spark/Outbrain_Project_PySpark/datasets/'\n",
    "a = sc.textFile(path+'clicks_test.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PK\\x03\\x04\\x14\\x00\\x02\\x00\\x08\\x00�]CIZk��>�v\\x08�~7\\x1e\\x0f\\x00\\x1c\\x00clicks_test.csvUT\\t\\x00\\x03�}�Wk}�Wux\\x0b\\x00\\x01\\x04�\\x01\\x00\\x00\\x04\\x14\\x00\\x00\\x00\\\\�I�,;Ҥ9�����\\'WSH��$��\\x02jT�/\\x15�q\\x15�@ċ/̏7fl�ڈ����\\x7f��������\\x7f��\\x7f��\\x7f���2������\\\\c\\x0b���\\u074cg]�\\x1b����\\\\�����:��㿽�\\x14�o~GXz;;�l�W����c\\x15�U\\\\muW�\\\\F��y��\\x1b��bm?|J�\\x0f���W��ں\\x15�~�b<�\\x07���w��5\\ue378ƻ�.n�X�s�/�7g�7W՝����%�x2v��1[�����*�2�Z�x���{���\\x1be:��������}q}|�x���\\x02\\x1ck�\"\\\\ߨG\\x18�5\\x7f(�ŭ�7+�K?�Y����kv�������������|��������4\\x1e��-�s�����2�%n��\\x7f���\\t��{���b�q']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['USE db_countries;', '-- IMPORTING DATA FROM A CSV']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuc(lines):\n",
    "    lines =lines.lower()\n",
    "    lines = lines.split()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = rdd.map(fuc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['use', 'db_countries;'], ['--', 'importing', 'data', 'from', 'a', 'csv']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['use', 'db_countries;', '--', 'importing', 'data']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = rdd.flatMap(fuc)\n",
    "rdd2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['db_countries;', '--']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=['a', 'use', 'the', 'is']\n",
    "rdd3 = rdd2.filter(lambda x: x not in stopwords)\n",
    "rdd3.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('db_', <pyspark.resultiterable.ResultIterable at 0x114eb5fd0>),\n",
       " ('--', <pyspark.resultiterable.ResultIterable at 0x114eb5f90>),\n",
       " ('imp', <pyspark.resultiterable.ResultIterable at 0x114eb9250>),\n",
       " ('dat', <pyspark.resultiterable.ResultIterable at 0x114eb90d0>),\n",
       " ('fro', <pyspark.resultiterable.ResultIterable at 0x114eb93d0>)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4 = rdd3.groupBy(lambda x: x[0:3])\n",
    "rdd4.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('db_', ['db_countries;']), ('--', ['--', '--', '--']), ('imp', ['importing']), ('dat', ['data']), ('fro', ['from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from', 'from'])]\n"
     ]
    }
   ],
   "source": [
    "print([(k,list(v)) for (k, v) in rdd4.take(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742 294\n"
     ]
    }
   ],
   "source": [
    "rdd3_sample = rdd3.sample(False, 0.4, 42)\n",
    "print(len(rdd3.collect()), len(rdd3_sample.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['db_countries;', 'from']"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1 = rdd3_sample.sample(False, 0.2, 42)\n",
    "sample2 = rdd3_sample.sample(False, 0.2, 42)\n",
    "sample2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_sample = sample1.join(sample2)\n",
    "# join_sample.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1171] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd = sc.parallelize(range(1,5))\n",
    "num_rdd.reduce(lambda x,y: x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x114e7c990>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sparkconf\n",
    "import pyspark\n",
    "rmowers = sqlContext.read.csv('file:///Users/tjmask/Desktop/Semester2/Spark/PySpark/Datasets/RidingMowers.csv', \\\n",
    "                              inferSchema='true', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Income', 'Lot_Size', 'Ownership']"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmowers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Lot_Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.0</td>\n",
       "      <td>18.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43.2</td>\n",
       "      <td>20.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47.4</td>\n",
       "      <td>16.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49.2</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>51.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>52.8</td>\n",
       "      <td>20.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Income  Lot_Size\n",
       "0    33.0      18.8\n",
       "1    43.2      20.4\n",
       "2    47.4      16.4\n",
       "3    49.2      17.6\n",
       "4    51.0      14.0\n",
       "5    51.0      22.0\n",
       "6    52.8      20.8"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmowers.orderBy(['Income', 'Lot_Size'], ascending = True).limit(7).toPandas()[['Income', 'Lot_Size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "owner_groupby = rmowers.groupBy('Ownership')\\\n",
    "                .sum('Income')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Ownership='Owner', sum(Income)=953.6999999999999),\n",
       " Row(Ownership='Nonowner', sum(Income)=688.8)]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "owner_groupby.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+\n",
      "|Income|Lot_Size|Ownership|\n",
      "+------+--------+---------+\n",
      "|  60.0|    18.4|    Owner|\n",
      "|  85.5|    16.8|    Owner|\n",
      "|  64.8|    21.6|    Owner|\n",
      "|  61.5|    20.8|    Owner|\n",
      "|  87.0|    23.6|    Owner|\n",
      "+------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmowers.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Income: double (nullable = true)\n",
      " |-- Lot_Size: double (nullable = true)\n",
      " |-- Ownership: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmowers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmowers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|Income|Ownership|\n",
      "+------+---------+\n",
      "|  60.0|    Owner|\n",
      "|  85.5|    Owner|\n",
      "|  64.8|    Owner|\n",
      "|  61.5|    Owner|\n",
      "|  87.0|    Owner|\n",
      "+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmowers.select('Income', 'Ownership').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            Income|\n",
      "+-------+------------------+\n",
      "|  count|                24|\n",
      "|   mean|           68.4375|\n",
      "| stddev|19.793143575710648|\n",
      "|    min|              33.0|\n",
      "|    max|             110.1|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmowers.describe('Income').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+\n",
      "|Income|Lot_Size|Ownership|\n",
      "+------+--------+---------+\n",
      "| 110.1|    19.2|    Owner|\n",
      "| 108.0|    17.6|    Owner|\n",
      "|  93.0|    20.8|    Owner|\n",
      "+------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmowers.filter(rmowers.Income >= '90').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+\n",
      "|Income|Lot_Size|Ownership|\n",
      "+------+--------+---------+\n",
      "|  60.0|    18.4|    Owner|\n",
      "|  85.5|    16.8|    Owner|\n",
      "|  64.8|    21.6|    Owner|\n",
      "|  61.5|    20.8|    Owner|\n",
      "|  87.0|    23.6|    Owner|\n",
      "+------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rmowers.where((rmowers.Income >= 60) & (rmowers.Ownership=='Owner')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmowers.registerTempTable('rmowers_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+\n",
      "|Income|Lot_Size|Ownership|\n",
      "+------+--------+---------+\n",
      "|  60.0|    18.4|    Owner|\n",
      "|  85.5|    16.8|    Owner|\n",
      "|  64.8|    21.6|    Owner|\n",
      "|  61.5|    20.8|    Owner|\n",
      "|  87.0|    23.6|    Owner|\n",
      "+------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select * from rmowers_df').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|min_income|\n",
      "+----------+\n",
      "|      33.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select min(Income) min_income from rmowers_df').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+\n",
      "|Income|Lot_Size|Ownership|\n",
      "+------+--------+---------+\n",
      "|  33.0|    18.8| Nonowner|\n",
      "+------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select * from rmowers_df where Income in (select min(Income) min_income from rmowers_df)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster('local[*]').setAppName('first-ML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x114f55250>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "toyota = pd.read_csv('/Users/tjmask/Desktop/Semester2/Spark/PySpark/Datasets/ToyotaCorolla.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Model</th>\n",
       "      <th>Price</th>\n",
       "      <th>Age_08_04</th>\n",
       "      <th>Mfg_Month</th>\n",
       "      <th>Mfg_Year</th>\n",
       "      <th>KM</th>\n",
       "      <th>Fuel_Type</th>\n",
       "      <th>HP</th>\n",
       "      <th>Met_Color</th>\n",
       "      <th>...</th>\n",
       "      <th>Powered_Windows</th>\n",
       "      <th>Power_Steering</th>\n",
       "      <th>Radio</th>\n",
       "      <th>Mistlamps</th>\n",
       "      <th>Sport_Model</th>\n",
       "      <th>Backseat_Divider</th>\n",
       "      <th>Metallic_Rim</th>\n",
       "      <th>Radio_cassette</th>\n",
       "      <th>Parking_Assistant</th>\n",
       "      <th>Tow_Bar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors</td>\n",
       "      <td>13500</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>2002</td>\n",
       "      <td>46986</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors</td>\n",
       "      <td>13750</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>2002</td>\n",
       "      <td>72937</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                          Model  Price  Age_08_04  \\\n",
       "0   1  TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  13500         23   \n",
       "1   2  TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  13750         23   \n",
       "\n",
       "   Mfg_Month  Mfg_Year     KM Fuel_Type  HP  Met_Color  ... Powered_Windows  \\\n",
       "0         10      2002  46986    Diesel  90          1  ...               1   \n",
       "1         10      2002  72937    Diesel  90          1  ...               0   \n",
       "\n",
       "   Power_Steering  Radio  Mistlamps  Sport_Model  Backseat_Divider  \\\n",
       "0               1      0          0            0                 1   \n",
       "1               1      0          0            0                 1   \n",
       "\n",
       "   Metallic_Rim  Radio_cassette  Parking_Assistant  Tow_Bar  \n",
       "0             0               0                  0        0  \n",
       "1             0               0                  0        0  \n",
       "\n",
       "[2 rows x 39 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyota.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1436, 39)\n"
     ]
    }
   ],
   "source": [
    "print(toyota.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "toyota_new = toyota.drop(columns =['Id', 'Model', 'Fuel_Type', 'Color']).dropna()\n",
    "# toyota_new.iloc[:, 34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "toyota_new.to_csv('/Users/tjmask/Desktop/Semester2/Spark/PySpark/Datasets/toyota_new.txt',\\\n",
    "                  sep=\",\", index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = sc.textFile('file:///Users/tjmask/Desktop/Semester2/Spark/PySpark/Datasets/toyota_new.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['13500,23,10,2002,46986,90,1,0,2000,3,4,5,210,1165,0,1,3,1,1,1,0,0,1,0,1,1,1,0,0,0,1,0,0,0,0']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_point(line):\n",
    "    values = [s for s in line.strip().split(',')]\n",
    "    return LabeledPoint(values[34], values[0:33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [13500.0,23.0,10.0,2002.0,46986.0,90.0,1.0,0.0,2000.0,3.0,4.0,5.0,210.0,1165.0,0.0,1.0,3.0,1.0,1.0,1.0,0.0,0.0,1.0,0.0,1.0,1.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [13750.0,23.0,10.0,2002.0,72937.0,90.0,1.0,0.0,2000.0,3.0,4.0,5.0,210.0,1165.0,0.0,1.0,3.0,1.0,1.0,1.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [13950.0,24.0,9.0,2002.0,41711.0,90.0,1.0,0.0,2000.0,3.0,4.0,5.0,210.0,1165.0,1.0,1.0,3.0,1.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [14950.0,26.0,7.0,2002.0,48000.0,90.0,0.0,0.0,2000.0,3.0,4.0,5.0,210.0,1165.0,1.0,1.0,3.0,1.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0]),\n",
       " LabeledPoint(0.0, [13750.0,30.0,3.0,2002.0,38500.0,90.0,0.0,0.0,2000.0,3.0,4.0,5.0,210.0,1170.0,1.0,1.0,3.0,1.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0,1.0,1.0,0.0,1.0,0.0,1.0,0.0,0.0])]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_parsed = points.map(parse_point)\n",
    "data_parsed.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data_parsed.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = DecisionTree.trainClassifier(train, numClasses=5, categoricalFeaturesInfo= {}, maxDepth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = train_model.predict(test.map(lambda x: x.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (0.0, 0.0),\n",
       " (1.0, 0.0),\n",
       " (0.0, 0.0)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LabelsAndPrediction = test.map(lambda x: x.label).zip(predictions)\n",
    "LabelsAndPrediction.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testErr = LabelsAndPrediction.map(lambda x: x.keys()=x.values()).count()/(predictions.count())\n",
    "# # testErr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = LabelsAndPrediction.keys().collect()\n",
    "values = LabelsAndPrediction.values().collect()\n",
    "count=0\n",
    "for i in range(410):\n",
    "    if keys[i] == values[i]:\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2804878048780488"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testErr = 1 - count/(predictions.count())\n",
    "testErr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeModel classifier of depth 3 with 7 nodes\n",
      "  If (feature 1 <= 31.5)\n",
      "   If (feature 4 <= 33403.0)\n",
      "    Predict: 0.0\n",
      "   Else (feature 4 > 33403.0)\n",
      "    If (feature 27 <= 0.5)\n",
      "     Predict: 0.0\n",
      "    Else (feature 27 > 0.5)\n",
      "     Predict: 1.0\n",
      "  Else (feature 1 > 31.5)\n",
      "   Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LabelsAndPrediction.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 3), ('c', 2), ('a', 1), ('d', 2), ('b', 1), ('d', 3)]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_key = sc.parallelize([('a', 1),('b', 3),('c', 2),('a', 1),('d', 2),('b', 1),('d', 3)],4)\n",
    "# data_key.reduceByKey(lambda x, y: x + y).collect()\n",
    "data_key.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', False), ('c', 2), ('a', True), ('d', False)]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_key.reduceByKey(lambda x, y: x==y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[281] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LabelsAndPrediction.filter(lambda x, y: x==y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(LabelsAndPrediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yinshihan', 1),\n",
       " ('tangjie', 1),\n",
       " ('yinshihan', 1),\n",
       " ('yinshihan', 1),\n",
       " ('tangjie', 1)]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## count words\n",
    "x = ['yinshihan', 'tangjie', 'yinshihan', 'yinshihan', 'tangjie']\n",
    "rdd = sc.parallelize(x)\n",
    "rdd0 = rdd.map(lambda word: (word,1))\n",
    "rdd0.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yinshihan', 3), ('tangjie', 2)]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = rdd0.reduceByKey(lambda a,b: a+b)\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read file from local& HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "rdd = sc.textFile('file:///Users/tjmask/Desktop/test.txt')\n",
    "rdd0 = rdd.flatMap(lambda line: re.split(r'[, /^\\][''|();>=-]', line))\n",
    "rdd1 = rdd0.map(lambda word: (word,1))\n",
    "rdd1 = rdd1.reduceByKey(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('USE', 1),\n",
       " ('db_countries', 1),\n",
       " ('', 661),\n",
       " ('IMPORTING', 1),\n",
       " ('DATA', 1),\n",
       " ('FROM', 79),\n",
       " ('A', 4),\n",
       " ('CSV', 1),\n",
       " ('select', 1),\n",
       " ('*', 34),\n",
       " ('countries_of_the_world', 25),\n",
       " ('SHOW', 1),\n",
       " ('TABLES', 1),\n",
       " ('DESCRIBE', 5),\n",
       " ('continents', 10),\n",
       " ('cities', 3),\n",
       " ('countries', 37),\n",
       " ('attractions', 8),\n",
       " ('BASIC', 1),\n",
       " ('SELECT', 78),\n",
       " ('STATEMENTS', 1),\n",
       " ('ORDER', 14),\n",
       " ('BY', 23),\n",
       " ('area', 2),\n",
       " ('WHERE', 48),\n",
       " ('area<647500', 2),\n",
       " ('DESC', 8),\n",
       " ('COUNTRY', 1),\n",
       " ('continents.NAME', 5),\n",
       " ('continents.SURFACE', 2),\n",
       " ('AS', 10),\n",
       " ('`Name', 3),\n",
       " ('of', 4),\n",
       " ('the', 4),\n",
       " ('Country`', 3),\n",
       " ('`Size', 2),\n",
       " ('in', 3),\n",
       " ('Squared', 3),\n",
       " ('Miles`', 1),\n",
       " ('1.8*continents.SURFACE', 2),\n",
       " ('Kilometers`', 2),\n",
       " ('ROUND', 1),\n",
       " ('1000', 1),\n",
       " ('2', 4),\n",
       " ('`', 1),\n",
       " ('This', 1),\n",
       " ('is', 1),\n",
       " ('just', 1),\n",
       " ('an', 1),\n",
       " ('example', 1),\n",
       " ('Size', 1),\n",
       " ('SURFACE', 2),\n",
       " ('`SIZE', 1),\n",
       " ('IN', 2),\n",
       " ('MILES`', 1),\n",
       " ('surface', 12),\n",
       " ('population', 40),\n",
       " ('ID_COUNTRY', 2),\n",
       " ('10', 10),\n",
       " ('NAME', 2),\n",
       " ('`name', 1),\n",
       " ('country`', 1),\n",
       " ('1', 4),\n",
       " ('name', 34),\n",
       " (\"'Spain'\", 2),\n",
       " (\"'SPAIN'\", 1),\n",
       " (\"'spAIN'\", 1),\n",
       " ('50e6', 1),\n",
       " ('population*0.0001', 1),\n",
       " ('20', 5),\n",
       " ('population*1e', 1),\n",
       " ('3', 2),\n",
       " ('BETWEEN', 12),\n",
       " ('AND', 13),\n",
       " ('60e6', 3),\n",
       " ('0', 3),\n",
       " ('200000', 1),\n",
       " ('LIMIT', 7),\n",
       " ('70e6', 2),\n",
       " ('OFFSET', 2),\n",
       " ('4', 2),\n",
       " ('00', 3),\n",
       " ('20000000', 2),\n",
       " ('2E9', 1),\n",
       " ('5', 2),\n",
       " ('continent', 19),\n",
       " ('2000000', 1),\n",
       " ('#', 4),\n",
       " ('DISTINCT', 1),\n",
       " ('COUNT', 2),\n",
       " ('AGGREGATIONS', 1),\n",
       " ('MAX', 4),\n",
       " ('SUM', 2),\n",
       " ('GROUP', 9),\n",
       " ('variance', 1),\n",
       " ('std', 1),\n",
       " ('Population', 1),\n",
       " ('MIN', 1),\n",
       " ('Region', 6),\n",
       " ('count', 1),\n",
       " ('Country', 14),\n",
       " ('Number_of_countries', 1),\n",
       " ('Basic', 1),\n",
       " ('String', 1),\n",
       " ('Manipulation', 1),\n",
       " ('LIKE', 11),\n",
       " (\"'%square%'\", 2),\n",
       " (\"'A%'\", 4),\n",
       " (\"'B%'\", 1),\n",
       " (\"'_%'\", 1),\n",
       " (\"'______%'\", 1),\n",
       " (\"'______'\", 1),\n",
       " ('country', 7),\n",
       " ('GDP', 3),\n",
       " ('avg', 6),\n",
       " (\"'China%'\", 1),\n",
       " ('or', 1),\n",
       " ('REGEXP', 13),\n",
       " (\"'\", 15),\n",
       " (\"A'\", 2),\n",
       " (\"'mar'\", 1),\n",
       " (\"mar'\", 1),\n",
       " (\"'China'\", 1),\n",
       " ('China', 1),\n",
       " (\"Spain'\", 1),\n",
       " (\"Denmark'\", 1),\n",
       " ('Denmark', 1),\n",
       " ('ABC', 1),\n",
       " ('Den', 2),\n",
       " ('HAVING', 3),\n",
       " ('round', 2),\n",
       " ('Pop_Density', 5),\n",
       " (\"'Average\", 1),\n",
       " (\"Population'\", 1)]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5]]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[range(1, 1), range(1, 2), range(1, 3), range(1, 4), range(1, 5)]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda i: range(1,i)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 1, 2, 3, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMap(lambda i: range(1,i)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reduce, fold, aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "rdd.reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008333333333333333"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add(a,b):\n",
    "    return a/b\n",
    "\n",
    "rdd.reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero=0\n",
    "# op = lambda x,u : x+y\n",
    "# rdd.fold(zero,op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroValue=0\n",
    "seqOp = lambda a, b:a+b\n",
    "combOp = lambda x,y:x+y\n",
    "rdd.aggregate(zeroValue, seqOp, combOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5]]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 2, 3, 4, 3, 4, 5, 4, 5, 6, 5, 6, 7]"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## filter and distinct\n",
    "rdd1 = rdd.flatMap(lambda x: (x, x+1, x+2))\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 4, 4, 4, 6, 6]"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.filter(lambda x: x%2==0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd3.map(t=>(t,null)).reduceByKey(x,y=>x).map(t=>t._1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intersextion, union, sortBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'a']"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd0= sc.parallelize(['c','a','b','b'])\n",
    "rdd1 = sc.parallelize(['a','a','b','b','d'])\n",
    "rdd2 = rdd0.intersection(rdd1)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c', 'a', 'b', 'b', 'a', 'a', 'b', 'b', 'd']"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3= rdd0.union(rdd1)\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d', 'c', 'b', 'b', 'b', 'b', 'a', 'a', 'a']"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.sortBy(lambda x:x).collect()\n",
    "rdd3.sortBy(lambda x:x, ascending=False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pairRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yinshihan', 1),\n",
       " ('tangjie', 1),\n",
       " ('yinshihan', 1),\n",
       " ('tangjie', 1),\n",
       " ('yinshihan', 1),\n",
       " ('tangjie', 1),\n",
       " ('yinshihan', 1),\n",
       " ('tangjie', 1),\n",
       " ('yinshihan', 1),\n",
       " ('tangjie', 1),\n",
       " ('yinshihan', 1),\n",
       " ('yinshihan', 1),\n",
       " ('tangjie', 1),\n",
       " ('yinshihan', 1)]"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## groupBykey, reduceByKey, aggregateByKey\n",
    "y = ['yinshihan', 'tangjie','yinshihan', 'tangjie','yinshihan', 'tangjie','yinshihan', \\\n",
    "     'tangjie','yinshihan', 'tangjie','yinshihan', 'yinshihan', 'tangjie','yinshihan']\n",
    "\n",
    "rdd00 = sc.parallelize(y)\n",
    "rdd00 = rdd00.map(lambda word: (word,1))\n",
    "rdd00.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yinshihan', <pyspark.resultiterable.ResultIterable at 0x117d59190>),\n",
       " ('tangjie', <pyspark.resultiterable.ResultIterable at 0x117d59210>)]"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd11 = rdd00.groupByKey()\n",
    "rdd11.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yinshihan', 8), ('tangjie', 6)]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd22= rdd00.reduceByKey(lambda a, b: a+b)\n",
    "rdd22.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yinshihan', 8), ('tangjie', 6)]"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroValue = 0\n",
    "seqFunc = lambda x,y:x+y\n",
    "combFunc = lambda x,y:x+y\n",
    "rdd3 = rdd00.aggregateByKey(zeroValue,seqFunc,combFunc)\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tangjie': 6, 'yinshihan': 8}"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd44 = rdd00.reduceByKeyLocally(lambda x,y:x+y)\n",
    "rdd44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.7.29:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>First Lesson</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=First Lesson>"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf =  SparkConf().setMaster('local[*]').setAppName('ysh')\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = [1,2,3,4,5]\n",
    "rdd = sc.parallelize(m)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'hdfs://TJmaskdeMacBook-Pro.local:8080/Users/tjmask/Desktop/test.txt'\n",
    "path='file:///Users/tjmask/Desktop/test.txt'\n",
    "rdd = sc.textFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read multiple files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'file:///Users/tjmask/Desktop/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.wholeTextFiles(path)\n",
    "# rdd.take(1)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD transformation & action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map, flatMap, filter, distinct, sample, sortBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberRDD =  sc.parallelize(range(1,11))\n",
    "numberRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberRDD.map(lambda x: x**2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberRDD.filter(lambda x: x%2==0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 4, 3, 9, 4, 16, 5, 25, 6, 36, 7, 49, 8, 64, 9, 81, 10, 100]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatMapRDD = numberRDD.flatMap(lambda x:(x,x**2))\n",
    "flatMapRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(set(flatMapRDD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 3, 9, 16, 5, 25, 6, 36, 7, 49, 8, 64, 81, 10, 100]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberRDD.flatMap(lambda x:(x,x**2)).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,3,5,2,3,4]\n",
    "x.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 4, 5, 6, 10, 10, 10]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleRDD = numberRDD.sample(True, 0.7,4)\n",
    "sampleRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 81, 64, 49, 36, 25, 16, 10, 9, 9, 8, 7, 6, 5, 4, 4, 3, 2, 1, 1]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortRDD = flatMapRDD.sortBy(keyfunc=lambda x:x, ascending=False)\n",
    "sortRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odd(x):\n",
    "    if x%2==1:\n",
    "        return 2*x \n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 14, 10, 8]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberRDD\\\n",
    "        .map(odd)\\\n",
    "        .filter(lambda x: x>6)\\\n",
    "        .distinct()\\\n",
    "        .sortBy(keyfunc=lambda x:x, ascending=False)\\\n",
    "        .collect()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PairRDD: groupByKey, sortByKey, reduceByKey, aggregateByKey, sampleByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello hello ysh yinshihan', 'Hi Hello ysh TJ', 'Hi, yinshihan']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(['Hello hello ysh yinshihan', 'Hi Hello ysh TJ', 'Hi, yinshihan'])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 1),\n",
       " ('hello', 1),\n",
       " ('ysh', 1),\n",
       " ('yinshihan', 1),\n",
       " ('hi', 1),\n",
       " ('hello', 1),\n",
       " ('ysh', 1),\n",
       " ('tj', 1),\n",
       " ('hi', 1),\n",
       " ('', 1),\n",
       " ('yinshihan', 1)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "pairRDD = rdd.flatMap(lambda x: re.split(r'[ ,]',x))\\\n",
    "             .map(lambda x: x.lower())\\\n",
    "             .map(lambda word: (word,1))        \n",
    "pairRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', <pyspark.resultiterable.ResultIterable at 0x11671b110>),\n",
       " ('ysh', <pyspark.resultiterable.ResultIterable at 0x11671b510>),\n",
       " ('yinshihan', <pyspark.resultiterable.ResultIterable at 0x11671b190>),\n",
       " ('hi', <pyspark.resultiterable.ResultIterable at 0x11671b990>),\n",
       " ('tj', <pyspark.resultiterable.ResultIterable at 0x11671b8d0>),\n",
       " ('', <pyspark.resultiterable.ResultIterable at 0x11671bfd0>)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupRDD = pairRDD.groupByKey()\n",
    "groupRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 3), ('ysh', 2), ('yinshihan', 2), ('hi', 2), ('tj', 1), ('', 1)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduceRDD = pairRDD.reduceByKey(lambda x,y:x+y)\n",
    "reduceRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 1),\n",
       " ('hello', 1),\n",
       " ('hello', 1),\n",
       " ('hello', 1),\n",
       " ('hi', 1),\n",
       " ('hi', 1),\n",
       " ('tj', 1),\n",
       " ('yinshihan', 1),\n",
       " ('yinshihan', 1),\n",
       " ('ysh', 1),\n",
       " ('ysh', 1)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortRDD = pairRDD.sortByKey(ascending = True, keyfunc=lambda x:x)\n",
    "sortRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 3), ('ysh', 2), ('yinshihan', 2), ('hi', 2), ('tj', 1), ('', 1)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeroValue=0\n",
    "seqFunc = lambda x,y:x+y\n",
    "combFunc =lambda x,y:x+y\n",
    "aggregateRDD = pairRDD.aggregateByKey(zeroValue, seqFunc,combFunc)\n",
    "aggregateRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 1), ('ysh', 1), ('yinshihan', 1)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleRDD = pairRDD.sampleByKey(withReplacement=True, fractions=dict([('hello', 0.1), ('ysh', 0.2), ('yinshihan', 0.3), ('hi', 0.3), ('tj', 0.1), ('', 0)]))\n",
    "sampleRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('', 0),\n",
       " ('hello', 0.1),\n",
       " ('hi', 0.3),\n",
       " ('tj', 0.1),\n",
       " ('yinshihan', 0.3),\n",
       " ('ysh', 0.2)}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{('hello', 0.1), ('ysh', 0.2), ('yinshihan', 0.3), ('hi', 0.3), ('tj', 0.1), ('', 0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bar': 3, 'egg': 2, 'spam': 1}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict([('spam', 1), ('egg', 2), ('bar', 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### union, intersection, subtract, cartesian(正交和）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize([1,2,3])\n",
    "rdd2 = sc.parallelize([2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 2, 3, 4]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unionRDD = rdd1.union(rdd2)\n",
    "unionRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersectionRDD = rdd1.intersection(rdd2)\n",
    "intersectionRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtractRDD = rdd1.subtract(rdd2)\n",
    "subtractRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtractRDD = rdd2.subtract(rdd1)\n",
    "subtractRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (1, 3), (1, 4), (2, 2), (2, 3), (2, 4), (3, 2), (3, 3), (3, 4)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cartesianRDD = rdd1.cartesian(rdd2)\n",
    "cartesianRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join, leftOuterJoin, rightOuterJoin, fullOuterjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bob', ('jack', 10)),\n",
       " ('bob', ('john', 10)),\n",
       " ('ysh', ('tj', 520)),\n",
       " ('ysh', ('ysh', 520))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([('bob','jack'),('bob','john'), ('ysh','tj'),('ysh','ysh')])\n",
    "rdd2 = sc.parallelize([('bob',10),('ysh',520),('boss',521)])\n",
    "\n",
    "innerRDD = rdd1.join(rdd2)\n",
    "innerRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bob', ('jack', 10)),\n",
       " ('bob', ('john', 10)),\n",
       " ('ysh', ('tj', 520)),\n",
       " ('ysh', ('ysh', 520))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leftRDD = rdd1.leftOuterJoin(rdd2)\n",
    "leftRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bob', ('jack', 10)),\n",
       " ('bob', ('john', 10)),\n",
       " ('ysh', ('tj', 520)),\n",
       " ('ysh', ('ysh', 520)),\n",
       " ('boss', (None, 521))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rightRDD = rdd1.rightOuterJoin(rdd2)\n",
    "rightRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bob', ('jack', 10)),\n",
       " ('bob', ('john', 10)),\n",
       " ('ysh', ('tj', 520)),\n",
       " ('ysh', ('ysh', 520)),\n",
       " ('boss', (None, 521))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullRDD = rdd1.fullOuterJoin(rdd2)\n",
    "fullRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### action : collect, take, first\n",
    "executor 返回driver的api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = sc.textFile('file:///Users/tjmask/Desktop/test/test.txt')\\\n",
    "#         .map(lambda line: line.split(','))\\\n",
    "#         .map(lambda arr: (arr[0],arr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## way 2\n",
    "from pyspark import Row\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                    .master('local[*]')\\\n",
    "                    .appName('ysh_sql')\\\n",
    "                    .getOrCreate()\n",
    "\n",
    "rdd1 = sc.textFile('file:///Users/tjmask/Desktop/test/test.txt')\\\n",
    "        .map(lambda line: line.split(','))\\\n",
    "\n",
    "\n",
    "people = rdd1.map(lambda arr: Row[name=arr[0], age=arr[1]])\n",
    "\n",
    "df2 = spark.createDataFrame(people)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "from pyspark.sql.types import *\n",
    "person = rdd1.map(lambda arr: (arr[0], arr[1]))\n",
    "\n",
    "schema = StructType([StructField('name', StringType()),\n",
    "                    StructField('age', StringType()) ])\n",
    "\n",
    "\n",
    "\n",
    "df3  = spark.createDataFrame(person, schema)\n",
    "df4 = spark.read.schema(schema).json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write to Mysql\n",
    "url = 'jdbc:mysql://tjmask.com:8080/ysh1314'\n",
    "table = 'person'\n",
    "mode = 'overwrite'\n",
    "properties = {'user': 'root', 'password':'123456'}\n",
    "df3.write.jdbc(url=,mode=None, properties=None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output: foreach foreachPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullRDD.foreach(lambda x:x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saveAsTextFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import os, time\n",
    "from pysparkWordcount import conf\n",
    "from datetime import datetime\n",
    "\n",
    "# if 'SPARK_HOME' not in os.environ:\n",
    "#     os.environ['SPARK_HOME'] = '.....'\n",
    "#     os.environ['PYSPARK_PYTHON'] = '.....'\n",
    "# conf =  SparkConf().setMaster('local[*]').setAppName('ysh')\n",
    "\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "path = 'Datasets/RidingMowers.csv'\n",
    "rdd = sc.textFile(path)\n",
    "print(rdd.take(2))\n",
    "\n",
    "## word frequency\n",
    "result = rdd.flatMap(lambda line: line.split(',')).map(lambda word: (word,1)).reduceByKey(lambda a,b: a+b)\n",
    "\n",
    "## output\n",
    "# result.saveAsTextFile('Datasets/wordCount'+datetime.now().strftime('%Y-%m-%d'))\n",
    "\n",
    "def func(iter):\n",
    "    for i in iter:\n",
    "        print(i)\n",
    "result.foreachPartition(lambda iter:func(iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tian chi 天池, kesci和鲸，CCF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### handoop \n",
    "* hdfs : 分布式储存文件(distributed saving files)\n",
    "* 分布式并行计算引擎\n",
    "* Yarn 统一的通用的资源调度框架\n",
    "\n",
    "### hive\n",
    "    提供了sql的开发方式,结构化\n",
    "    \n",
    "### hbase\n",
    "    海量的非结构化数据储存， 半结构化（xml)\n",
    "    \n",
    "### sqoop/dataX\n",
    "    sql to handoop\n",
    "    \n",
    "### kibana, eharts, highchart, tableau, datav\n",
    "\n",
    "### hue\n",
    "    整合大数据所有的框架实现web操作\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spark\n",
    "    sparkcore 核心\n",
    "    sparksql 交互式查询 数据来源hive\n",
    "    sparkstreaming 流式计算\n",
    "    mllib/ml 分布式计算机器学习\n",
    "    graphX 图形之间的关系\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saveAsHadoopDataSet(jobConf())\n",
    "\n",
    "rdd.saveAsHadoopFile[TableOutputFormat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextOutputFormat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-a70e1947abfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# rdd.saveAsHadoopFile[TableoutputFormat]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msampleRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsHadoopFile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTextOutputFormat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'TextOutputFormat' is not defined"
     ]
    }
   ],
   "source": [
    "# rdd.saveAsHadoopFile[TableoutputFormat]\n",
    "sampleRDD.saveAsHadoopFile[TextOutputFormat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform: dstream transform to RDD\n",
    "\n",
    "dstream.foreachRDD(rdd=>{\n",
    "    \n",
    "})\n",
    "\n",
    "\n",
    "dstream.foreachRDD((rdd,time)=>{\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据收集层: flume, sqoop, kafka\n",
    "\n",
    "数据储存层: HDFS, hive, hbase, kafka\n",
    "\n",
    "数据处理层: MR, hive, spark\n",
    "\n",
    "数据展示层: tableau javaweb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MVN repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dstream transform and foreachRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
